{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kode-git/FER-Visual-Transformers/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_2mq1eMSZ2U"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czOtUmPBSf3y"
   },
   "source": [
    "In this notebook, we will balance dataset and prepares data for the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJNKImLjSpMi"
   },
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxpbpYfzSpyO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtMCled2Se5a",
    "outputId": "0614f7de-4b09-4caf-e07e-cbd203edcb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "llfUDcltSS-Q"
   },
   "outputs": [],
   "source": [
    "# classic libraries for collections.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# utility library.\n",
    "import random, time, copy, sys, shutil\n",
    "\n",
    "# plot libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# libraries for image processing.\n",
    "import os, cv2, glob, imageio, sys\n",
    "from PIL import Image\n",
    "\n",
    "# warning library for service warnings.\n",
    "import warnings\n",
    "\n",
    "# ImageDataGenerator from keras library.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# colab library.\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xq0TJW7BTDye",
    "outputId": "995df2bb-a5ec-449b-d8f7-e8b1e9508a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# load Google Drive environment.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAH3XrZVThHR"
   },
   "source": [
    "## Image Worker Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5-KgvV1TlRR"
   },
   "source": [
    "ImageWorker provides some useful functions:\n",
    "- Format Converter: For resize and move an image from *source_path* to *dest_path* filtered for *format_img*\n",
    "- List Classes: Listing the classes and put them in an array to manipulate the subfolders for class functions divisions.\n",
    "- Counter Samples per Class: Given a *dataset_path*, return a dictionary with counters of images classified by subfolders for plot or data visualization pourposes. \n",
    "- Counter Samples: Given a *dataset_path*, return a counter of images in the tree.\n",
    "- Extension Converter: Convert an image format for every image in a specified path\n",
    "- Counter Files Extension: Given a *path*, return the counter of image in the directory with a specific *format*\n",
    "- Navigate Path: Counter every file in a subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GAjyHYPpTnJq"
   },
   "outputs": [],
   "source": [
    "class ImageWorker():\n",
    "    \"\"\"\n",
    "    Image Worker class for Data Integration.\n",
    "    This class manages images data, size and format.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "         pass\n",
    "\n",
    "    def format_converter(self, path, format_img, source_path, dest_path, resize=(224,224)):\n",
    "        \"\"\"\n",
    "        Move an image from source_path to dest_path.\n",
    "        Images selected follow format_img.\n",
    "        There is a default resize of (224,244).\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for file in glob.glob(path + \"/*.\" + format_img):\n",
    "            img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "            resized = cv2.resize(img, resize, interpolation=cv2.INTER_CUBIC)\n",
    "            cv2.imwrite(dest_path + \"resized_on_\" + source_path + \"_\" + str(count) + \".\"+ format_img, resized)\n",
    "            count += 1\n",
    "\n",
    "    def list_classes(self, dataset_path):\n",
    "        \"\"\"\n",
    "        List the classes of a dataset.\n",
    "        \"\"\"\n",
    "        langs = []\n",
    "        for el in glob.glob(dataset_path):\n",
    "          langs.append(os.path.basename(str(el)))\n",
    "        return langs\n",
    "\n",
    "\n",
    "    def counter_samples_on_class(self, dataset_path):\n",
    "      \"\"\"\n",
    "      Counts samples of classes.\n",
    "      Each class has its own counter.\n",
    "      Return a dictionary with (class, counter) pair.\n",
    "      \"\"\"\n",
    "      classes = self.list_classes(dataset_path)\n",
    "      counter_classes = {}\n",
    "      if dataset_path[len(dataset_path) - 1] == \"/\":\n",
    "          path = dataset_path\n",
    "      if dataset_path[len(dataset_path) - 1] == \"*\":\n",
    "          path = dataset_path[0:len(dataset_path) - 2] + \"/\"\n",
    "      else:\n",
    "          path = dataset_path + \"/\"\n",
    "      for class_ in classes:\n",
    "        counter = 0\n",
    "        for file in glob.glob(path + class_ + \"/*\"):\n",
    "            counter += 1\n",
    "        counter_classes[class_] = counter\n",
    "      return counter_classes\n",
    "\n",
    "\n",
    "    def counter_samples(self, dataset_path):\n",
    "      \"\"\"\n",
    "      Counts total samples of a dataset.\n",
    "      \"\"\"\n",
    "      a = self.counter_samples_on_class(dataset_path)\n",
    "      counter = 0\n",
    "      for el in a.keys():\n",
    "        counter += a[el]\n",
    "      return counter\n",
    "\n",
    "\n",
    "    def extension_converter(self, path, format_source, format_result, dest_path):   \n",
    "      \"\"\"\n",
    "      Convert a file from format_source to format_result.\n",
    "      The file is loaded from path and the result is stored to dest_path.\n",
    "      \"\"\"\n",
    "      for file in glob.glob(path + \"/*.\" + format_source):\n",
    "          im1 = Image.open(file)\n",
    "          im1.save(file[0:len(file)-4] + \".\" + format_result)\n",
    "          os.remove(file)\n",
    "\n",
    "\n",
    "    def counter_file_extension(self, path, format):\n",
    "      \"\"\"\n",
    "      Counts samples in path based on format input.\n",
    "      \"\"\"\n",
    "      counter = 0\n",
    "      for file in glob.glob(path + \"/*.\" + format):\n",
    "          counter += 1\n",
    "      return counter\n",
    "      \n",
    "\n",
    "    def navigate_path(self, path):    \n",
    "      \"\"\"\n",
    "      Navigate in the path and counts every file\n",
    "      \"\"\"\n",
    "      count = 0\n",
    "      for dir in os.listdir(path):\n",
    "          if os.path.isfile(os.path.join(path, dir)):\n",
    "              count += 1\n",
    "      return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WUfP5Qw3T5U_"
   },
   "outputs": [],
   "source": [
    "# define Image Worker instance\n",
    "iw = ImageWorker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6MVdmlBTQSH"
   },
   "source": [
    "## Common utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vJkQvdlTZLx"
   },
   "source": [
    "We implemented some logic and reusable functions useful for the data analysis or data manipulation phases. These functions carry out support routines for ImageWorker's class. They are:\n",
    "- Min, Max and Mean: According to values or set of values passed as parameter.\n",
    "- Plot Dataset: Function for plot image's dataset and color values according to the mean of classes cardinalities.\n",
    "- Channel Distribution: Analyze images and return counters of images for different channels dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A4zHTE6HTLsw"
   },
   "outputs": [],
   "source": [
    "def mean(values):\n",
    "  \"\"\"\n",
    "  Calculates the mean in values.\n",
    "  \"\"\"\n",
    "  if len(values) <= 0:\n",
    "    return 0\n",
    "  else:\n",
    "    sum = 0\n",
    "    for el in values:\n",
    "      sum += el\n",
    "    return int(sum / len(values))\n",
    "\n",
    "def min(val):\n",
    "  \"\"\"\n",
    "  Calculates the minimum in val.\n",
    "  \"\"\"\n",
    "  min = sys.maxsize\n",
    "  for el in val.keys():\n",
    "    if val[el] < min:\n",
    "      min = val[el]\n",
    "  return min\n",
    "\n",
    "\n",
    "def max(val):\n",
    "  \"\"\"\n",
    "  Calculates the maximum in val.\n",
    "  \"\"\"\n",
    "  max = sys.minsize\n",
    "  for el in val.keys():\n",
    "    if val[el] > max:\n",
    "      max = val[el]\n",
    "  return max\n",
    "\n",
    "\n",
    "def plot_dataset(dataset_path, title=\"\"):\n",
    "  \"\"\"\n",
    "  Plot the dataset and color bars.\n",
    "  Color depends on the lower bound and upper bound.\n",
    "  The mean value is the congiuntion between lower and upper bound.\n",
    "  \"\"\"\n",
    "  classes = iw.list_classes(dataset_path)\n",
    "  l_classes = iw.counter_samples_on_class(dataset_path)\n",
    "\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_axes([0,0,1,1,])\n",
    "  x = [l_classes[class_] for class_ in classes]\n",
    "  y = [class_ for class_ in classes]\n",
    "  \n",
    "  colors = []\n",
    "  x_cap = mean(x)\n",
    "\n",
    "  # colors identify when the elements are greater or lesser than the mean values.\n",
    "  for el in x:\n",
    "    if el < x_cap:\n",
    "      colors.append(\"#BC3434\")\n",
    "    else:\n",
    "      colors.append(\"#49A131\")\n",
    "  ax.bar(y, x, color=colors)\n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dARJwo1NTsjD"
   },
   "source": [
    "## Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY_Hpr86TyQn"
   },
   "source": [
    "AVFER contains AffectNet in the validation and testing set and FER-2013 and CK+48 in the training set. We need to balance it remains only a small amount of samples in the val/test sets and put the rest in the training set. We need to balance every class in the testing and validation set before put the residual samples in the training folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X00VEUE8Tt4M"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/drive/MyDrive/Datasets/AVFER/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvdE7-pnT0SU"
   },
   "outputs": [],
   "source": [
    "# main variables.\n",
    "basedir = \"/content/drive/MyDrive/Datasets/AVFER/\"\n",
    "types = ['val', 'test', 'train']\n",
    "ref = [str(basedir  + types[i] + \"/*\") for i in range(len(types))]\n",
    "total = 0\n",
    "for el in ref:\n",
    "  total += iw.counter_samples(el)\n",
    "\n",
    "# Splitting ratio 100 :-> 80/20 (train/test) and 80 :-> 90/10 (train/val).\n",
    "train_ratio = 80\n",
    "val_ratio = 10\n",
    "test_ratio = 20\n",
    "\n",
    "# Splitting distribution.\n",
    "val_amount = int((((total * train_ratio) / 100) * 10 / 100))\n",
    "test_amount = int((total * test_ratio)/ 100)\n",
    "train_amount = int(total - (val_amount + test_amount))\n",
    "print('Amount of samples per class for validation set:', val_amount) \n",
    "print('Amount of samples per class for testing set:', test_amount) \n",
    "print('Amount of samples per class for training set:', train_amount) \n",
    "\n",
    "# check augmentation of 3. \n",
    "augm_train = train_amount * 3\n",
    "print('Amount of augmented training set: ', augm_train)\n",
    "print('Amount of samples for augmented training set divided by classes:', int(augm_train / 8))\n",
    "print('Amount of samples for validation set divided by classes:', int(val_amount / 8))\n",
    "print('Amount of samples for testing set divided by classes:', int(test_amount / 8))\n",
    "\n",
    "# validation balancing.\n",
    "val = ref[0]\n",
    "min = sys.maxsize\n",
    "countcl = iw.counter_samples_on_class(val)\n",
    "for el in countcl.keys():\n",
    "  if countcl[el] < min:\n",
    "    min = countcl[el]\n",
    "\n",
    "# controls on the minimum number.\n",
    "if min < (val_amount / 8):\n",
    "  print('Error, the amount of samples for the validation set can\\'t be reduce to minimum values')\n",
    "\n",
    "# updating residuals counters.\n",
    "classes = iw.list_classes(val)\n",
    "residual = {}\n",
    "for cl in classes:\n",
    "  residual[cl] = countcl[cl] - ((val_amount / 8) + (test_amount / 8))\n",
    "\n",
    "# create temporal classes directories\n",
    "for cl in classes:\n",
    "  os.mkdir(basedir + \"tmp/\" + cl + \"/\")\n",
    "\n",
    "print('-'*40)\n",
    "# check validation preconditions.\n",
    "print('Verify the correct amount for validation...')\n",
    "err = False\n",
    "for cl in classes:\n",
    "  if countcl[cl] - residual[cl] - (test_amount / 8) < (val_amount / 8):\n",
    "    print(f'Error, the residual amount put class {cl} to illegal value')\n",
    "    err = True\n",
    "\n",
    "if not err:\n",
    "  print('Splitting possible.')\n",
    "\n",
    "# splittig training and validation set according to the proportion previously calculated.\n",
    "valdir = basedir + ref[0] + \"/\"\n",
    "traindir = basedir + ref[2] + \"/\"\n",
    "for cl in classes:\n",
    "  c = 0\n",
    "  for fl in glob.glob(valdir + cl + \"/*\"):\n",
    "    if c < int(residual[cl]):\n",
    "      c += 1\n",
    "      shutil.copyfile(fl, os.path.join(traindir + cl, os.path.basename(fl)))\n",
    "      os.remove(os.path.join(valdir + cl, os.path.basename(fl)))\n",
    "    else:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVi7Ydm5Ur27"
   },
   "source": [
    "We put the residual samples of AffectNet from validation set to training set. The current amount of samples in the validation set is the sum of validation and testing validation. If we want to be sure that the splitting is doing correctly, we will execute the followings checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1b-5I33UBCn"
   },
   "outputs": [],
   "source": [
    "# check testing splitting preconditions.\n",
    "residual_test = {}\n",
    "print('Verify the correct amount for testing...')\n",
    "for cl in classes:\n",
    "  residual_test[cl] = countcl[cl] - residual[cl]\n",
    "  if residual_test[cl] - (test_amount / 8) < (val_amount / 8):\n",
    "    print(f'Error, the residual amount put class {cl} to illegal value')\n",
    "    err = True\n",
    "\n",
    "# check errors.\n",
    "if not err:\n",
    "  print('Splitting possible.')\n",
    "\n",
    "# shows the current status of the validation set.\n",
    "print('Current status of the validation set:')\n",
    "print('-'*60)\n",
    "valdir = ref[0]\n",
    "clcount = iw.counter_samples_on_class(valdir)\n",
    "res = 0\n",
    "\n",
    "# checks on classes.\n",
    "for cl in classes:\n",
    "  print(f'Total amount of {cl} samples in validation samples:', clcount[cl])\n",
    "  res = clcount[cl] - int((test_amount / 8))\n",
    "  print(f'Residual amount after last splitting {res}')\n",
    "print('-'*60)\n",
    "print(f'Final total amount of samples in the validation set:', res * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvT0lY0fU2W1"
   },
   "outputs": [],
   "source": [
    "# update directory references\n",
    "valdir = basedir + \"val\" + \"/\"\n",
    "testdir = basedir + \"tmp\" + \"/\"\n",
    "\n",
    "# splitting processing between validation and testing set for balance samples.\n",
    "for cl in classes:\n",
    "  c = 0\n",
    "  for fl in glob.glob(valdir + cl + \"/*\"):\n",
    "    if c < int((test_amount / 8)):\n",
    "      c += 1\n",
    "      shutil.copyfile(fl, os.path.join(testdir + cl, os.path.basename(fl)))\n",
    "      os.remove(os.path.join(valdir + cl, os.path.basename(fl)))\n",
    "    else:\n",
    "      break\n",
    "\n",
    "print('Splitting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHbeef8CVBDf"
   },
   "outputs": [],
   "source": [
    "# delete the subtree of th temporal directory.\n",
    "for cl in classes:\n",
    "  shutil.rmtree(basedir + \"tmp/\" + cl + \"/\")\n",
    "os.rmdir(basedir + \"tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXZznhS6VLC5"
   },
   "outputs": [],
   "source": [
    "# plot testing set.\n",
    "testdir = \"/content/drive/MyDrive/Datasets/AVFER/test\"\n",
    "plot_dataset(testdir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxVdTVpkVRRq"
   },
   "outputs": [],
   "source": [
    "# plot validation set.\n",
    "valdir = \"/content/drive/MyDrive/Datasets/AVFER/val\"\n",
    "plot_dataset(valdir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dDrmNsXVWid"
   },
   "outputs": [],
   "source": [
    "# plot training set.\n",
    "traindir = \"/content/drive/MyDrive/Datasets/AVFER/train\"\n",
    "plot_dataset(traindir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvEgy4Y3VZ7U"
   },
   "outputs": [],
   "source": [
    "# counter total samples of the training set.\n",
    "print('Tot. samples in the training set:', iw.counter_samples(traindir + \"/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVzxbA3MVmm6"
   },
   "source": [
    "To adjust that, we need to put more data in the training set taken from the validation set and try to underestimate the variation in the training phase. So, we can have more variety during the training phase and maintain high variance in well-formed images from AffectNet in the validation set and testing set to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piybkg2VVkfi"
   },
   "outputs": [],
   "source": [
    "# display the total amount of samples for each class.\n",
    "quantities = iw.counter_samples_on_class(valdir + \"/*\")\n",
    "for cl in quantities.keys():\n",
    "  print('Amount for class {} is {}'.format(cl, quantities[cl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Raon9RI4VzAw"
   },
   "outputs": [],
   "source": [
    "# display the future amount of data for each class with the respective residual value.\n",
    "counters = iw.counter_samples_on_class(valdir + \"/*\")\n",
    "for key in counters.keys():\n",
    "  print(f'The current amount for the class {key} is {counters[key]}')\n",
    "print('-'*40)\n",
    "print('Balance it as well as the testing set...')\n",
    "\n",
    "# residual is the remaining samples in the validation set.\n",
    "residual = 680\n",
    "\n",
    "# apply the balancing, moves images from validation set to training set until balancing.\n",
    "# residual is the result size of samples collections for each class in the validation set.\n",
    "for cl in glob.glob(valdir + \"/*\"):\n",
    "  total = counters[os.path.basename(cl)]\n",
    "  count = total - residual\n",
    "  print(\"{}: {} -> {} with residual of {}\".format(os.path.basename(cl), total, count, total - count ))\n",
    "  for fl in glob.glob(cl + \"/*\"):\n",
    "    if count != 0:\n",
    "      count = count - 1\n",
    "      shutil.copyfile(fl, os.path.join(traindir,os.path.basename(cl), os.path.basename(fl)))\n",
    "      os.remove(fl)\n",
    "    else:\n",
    "      break\n",
    "\n",
    "print('Balanced completed.')\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIB-fuvqWCyf"
   },
   "outputs": [],
   "source": [
    "# plot the result training set.\n",
    "traindir = \"/content/drive/MyDrive/Datasets/AVFER/train\"\n",
    "plot_dataset(traindir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSO4S8rsWfkg"
   },
   "outputs": [],
   "source": [
    "# plot the validation set.\n",
    "valdir = \"/content/drive/MyDrive/Datasets/AVFER/val\"\n",
    "plot_dataset(valdir + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-ZxhSoyWh_K"
   },
   "outputs": [],
   "source": [
    "# plot the testing set.\n",
    "testdir = \"/content/drive/MyDrive/Datasets/AVFER/test\"\n",
    "plot_dataset(testdir + \"/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1-RDrGGWovg"
   },
   "source": [
    "Unfortunately, the amount of data for the training set is unbalanced. So, we need to preprocess data of this subfolder using data augmentation and possible integration with additional data, especially for the disgust and contempt classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDcL2sH7WxLk"
   },
   "source": [
    "## Convert Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YT3y4cnW2lq"
   },
   "source": [
    "AVFER has some samples from CK+ as PNG and, generally, some images have 1 channel (gray-scaling). We will avoid the artificial coloring and reduce side-effects of the fooling image with only changes the number of channels and put the pixel value in RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIdIOIVDWmIS"
   },
   "outputs": [],
   "source": [
    "# base path for of the dataset.\n",
    "path = \"/content/drive/MyDrive/Datasets/AVFER/\"\n",
    "\n",
    "# checking the number of channels.\n",
    "total = [0,0] # 0 for 1 channel, 1 for 3 channels.\n",
    "for path in glob.glob(path + \"*\"):\n",
    "  for cl in glob.glob(path + \"/*\"):\n",
    "    counter = [0,0] # 0 for 1 channel, 1 for 3 channels.\n",
    "    for fl in glob.glob(cl + \"/*\"):\n",
    "      image = cv2.imread(fl)\n",
    "      if(len(image.shape)<2):\n",
    "        counter[0] += 1\n",
    "        conv = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if len(image.shape)==3:\n",
    "          cv2.imwrite(fl, image)\n",
    "      else:\n",
    "        counter[1] += 1\n",
    "    total = [total[i] + counter[i] for i in range(2)]\n",
    "\n",
    "print('Tot. number of channels, respectively, in gray-scales and RGB:', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-Upl3xAXLYo"
   },
   "source": [
    "In general, elements in the gray-scale mode are FER-2013 and CK+48 samples. So we need to convert them to RGB (3 channels). In the gray-scale samples, each pixel has 1 byte (8 bit equals to a value from 0 to 255, corresponding of the brighness value of the image as described in the gray-scale representation). During the transformation, we need to convert the byte associated to one pixel in 3 channels representation corresponding to 3 bytes equals to the red, green and blue values. In this convertion, each channel has the same value of the brighness to mantain the gray tone of colored pixel without value perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb053Wf8XI6U"
   },
   "outputs": [],
   "source": [
    "# base path of the dataset.\n",
    "path = \"/content/drive/MyDrive/Datasets/AVFER/\"\n",
    "total = [0,0] # 0 for 1 channel, 1 for 3 channels.\n",
    "\n",
    "# check changes after formatting.\n",
    "for path in glob.glob(path + \"*\"):\n",
    "  for cl in glob.glob(path + \"/*\"):\n",
    "    counter = [0,0] # 0 for 1 channel, 1 for 3 channels.\n",
    "    for fl in glob.glob(cl + \"/*\"):\n",
    "      image = cv2.imread(fl)\n",
    "      if(len(image.shape)<2):\n",
    "        counter[0] += 1\n",
    "      else:\n",
    "        counter[1] += 1\n",
    "    total = [total[i] + counter[i] for i in range(2)]\n",
    "\n",
    "print('Tot. number of channels, respectively, in gray-scales and RGB:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vpBfvDbXXYa",
    "outputId": "e43bd106-1f44-4ca2-d4a6-be869596355c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gray-scale mode: L\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('/content/drive/MyDrive/Datasets/FER-2013/train/anger/Training_10118481.jpg')\n",
    "# L is gray-scale.\n",
    "print('Gray-scale mode:', image.mode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8jY_QSrXjUi"
   },
   "outputs": [],
   "source": [
    "# test image in L mode.\n",
    "test = '/content/drive/MyDrive/Datasets/FER-2013/train/anger/Training_10118481.jpg'\n",
    "pic = imageio.imread(test)\n",
    "image = Image.open(test)\n",
    "\n",
    "# convert to RGB.\n",
    "image = image.convert('RGB')\n",
    "im_rgb = cv2.cvtColor(pic, cv2.COLOR_BGR2RGB)\n",
    "# temporal saving .\n",
    "tmp = '/content/test.jpg'\n",
    "image.save(tmp)\n",
    "pic2 = imageio.imread(tmp)\n",
    "\n",
    "# remove temporal image.\n",
    "os.remove(tmp)\n",
    "\n",
    "# plot the result and compare with converted RGB and original version.\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=2, figsize=(15,5)) \n",
    "ax1, ax2 = ax\n",
    "ax1.imshow(im_rgb[ :, :])\n",
    "ax2.imshow(pic2[:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSnOw1EMXpDf"
   },
   "outputs": [],
   "source": [
    "# display array of pixel values for a colored image.\n",
    "pic3 = imageio.imread(\"/content/drive/MyDrive/Datasets/AffectNet/train_class/anger/Copia di image0000006.jpg\")\n",
    "pic3[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSi-c7X1X88b"
   },
   "outputs": [],
   "source": [
    "# display rray of pixels values for 3 channels adaptation of the gray-scale image.\n",
    "pic2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfyzbAUhX3ez"
   },
   "outputs": [],
   "source": [
    "# training dataset reference.\n",
    "ref = \"/content/drive/MyDrive/Datasets/AVFER/train\"\n",
    "\n",
    "# automatic convert frm .png to .jpg (only on Google Drive)\n",
    "for cl in glob.glob(ref + \"/*\"):\n",
    "  print(\"Convert from class {}\".format(cl))\n",
    "  for fl in glob.glob(cl + \"/*\"):\n",
    "    if fl[len(fl)-4:len(fl)] == '.png':\n",
    "      os.rename(fl, str(fl[0:len(fl)-4]) + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WooPUz8eYUb7"
   },
   "outputs": [],
   "source": [
    "# counting of possible residual .png images after convertion.\n",
    "counter = 0\n",
    "for cl in glob.glob(ref + \"/*\"):\n",
    "  print(\"Counting png on class {}\".format(cl))\n",
    "  for fl in glob.glob(cl + \"/*\"):\n",
    "    if fl[len(fl)-4:len(fl)] == '.png':\n",
    "      counter += 1\n",
    "\n",
    "# show the result.\n",
    "print('Remaining png samples: {}'.format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ON52Ja7alnQ"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHQClAt_Yhcm"
   },
   "source": [
    "Renaming the source folders for augmentation, only train is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be0oIhmEYeBu"
   },
   "outputs": [],
   "source": [
    "# renaming files pre-augmentation.\n",
    "traindir = \"/content/drive/MyDrive/Datasets/AVFER/train\"\n",
    "for cl in glob.glob(traindir + \"/*\"):\n",
    "  counter = 0\n",
    "  for img in glob.glob(cl + \"/*\"):\n",
    "      os.rename(img, cl + \"/\" + os.path.basename(cl) + \"-\" + str(counter) + \"-file\" + img[len(img) - 4 : len(img)])\n",
    "      counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBOc7K6xY_OP"
   },
   "outputs": [],
   "source": [
    "def generator(path, \n",
    "              format_img, \n",
    "              dest_path,\n",
    "              starting_counter=0, \n",
    "              num_augment=1, \n",
    "              zoom_range=0.6, \n",
    "              brightness_range=(0.2,0.8),\n",
    "              width_shift_range=0.2,\n",
    "              height_shift_range=0.2,\n",
    "              rotation_range=10\n",
    "              ):\n",
    "   \"\"\"\n",
    "   Generate images with augmentation parameters specified in input.\n",
    "   Zero parameters avoids a specific technique application.\n",
    "   The generator can do strong augmentation according to the num_augment value.\n",
    "   \"\"\"\n",
    "   # generator declaration.\n",
    "   gen = ImageDataGenerator(\n",
    "            featurewise_center=True,\n",
    "            featurewise_std_normalization=True,\n",
    "            rotation_range=rotation_range,\n",
    "            width_shift_range=width_shift_range,\n",
    "            height_shift_range=height_shift_range,\n",
    "            brightness_range=brightness_range,\n",
    "            zoom_range=zoom_range,\n",
    "            horizontal_flip=True\n",
    "            )\n",
    "   print(f'Data Augmentation parameters:\\nZoom Range: {zoom_range}\\nBrighness Range: {brightness_range}\\nShift: ({width_shift_range},{height_shift_range})\\nRotation Degrees: {rotation_range}')\n",
    "   \n",
    "   # make directory in case it doesn't exist.\n",
    "   if iw.navigate_path(dest_path) == 0:\n",
    "      try:\n",
    "        os.mkdir(dest_path)\n",
    "      except FileExistsError:\n",
    "        pass\n",
    "   num_el = iw.navigate_path(dest_path)\n",
    "   c = starting_counter\n",
    "   print('Starting generation...')\n",
    "   for file in glob.glob(path + \"/*.\" + format_img):\n",
    "      img = cv2.imread(file)\n",
    "      # convert to numpy array.\n",
    "      # expand dimension to one sample.\n",
    "      samples = np.expand_dims(img, 0)\n",
    "      iterator = gen.flow(samples, batch_size=1)\n",
    "      for i in range(0, num_augment):\n",
    "          batch = iterator.next()\n",
    "          image = batch[0].astype('uint8')\n",
    "          cv2.imwrite(dest_path + \"augmented_on_\" + str(num_el) + \"_\" + str(c) + \".\"+ format_img, image)\n",
    "          c += 1\n",
    "   print(f'Data Augmentation for the {path} is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NJ3VWcrZcNp"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/drive/MyDrive/Datasets/AVFER/train/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNsa7mHxZg5S"
   },
   "source": [
    "The total amount of copies for each samples depends on the total amount of images for its class. We can calculate the number of copies N as follow:\n",
    "$ C(i) = T \\div N(i) + 1 $ $ \\forall i \\in classes.keys()$\n",
    "\n",
    "With $ T $ the total amount of samples that we want for each class, it is equal to 20.000, $ N(i) $ the initial amount of samples for the class $ i $ and $C(i)$ the final amount of data of the class $ i $ after the augmentation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Umni8Zn0Zeph"
   },
   "outputs": [],
   "source": [
    "# utilities variables.\n",
    "augm_class = []\n",
    "copies = {}\n",
    "\n",
    "# counters of samples in each class of the training set. \n",
    "categories_data = iw.counter_samples_on_class(\"/content/drive/MyDrive/Datasets/AVFER/train/*\")\n",
    "\n",
    "# defines the number of copies weighted to the current amount of data of the class samples.\n",
    "for cl in categories_data.keys():\n",
    "  copies[cl] = int(20000/categories_data[cl]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXndH2ewZwIp"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# applying weighted data augmentation.\n",
    "for class_ in categories_data.keys():\n",
    "  print('Generation of {} samples with {} copy...'.format(class_, copies[class_]))\n",
    "  generator(\"/content/drive/MyDrive/Datasets/AVFER/train/\" + class_, \"jpg\", \"/content/drive/MyDrive/Datasets/AVFER/train/\" + class_ + \"/\", 0, num_augment=copies[class_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXsWkZQ5Z2lh"
   },
   "outputs": [],
   "source": [
    "# making of the result and augmented dataset version. VFER is the augmented version of AVFER.\n",
    "!rm -rf /content/drive/MyDrive/Datasets/VFER/\n",
    "!mkdir /content/drive/MyDrive/Datasets/VFER/\n",
    "!mkdir /content/drive/MyDrive/Datasets/VFER/train\n",
    "!mkdir /content/drive/MyDrive/Datasets/VFER/val\n",
    "!mkdir /content/drive/MyDrive/Datasets/VFER/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl5pw-6WaBiN"
   },
   "outputs": [],
   "source": [
    "# creates subfolders for VFER.\n",
    "base_dir = \"/content/drive/MyDrive/Datasets/VFER/\"\n",
    "x = \"/content/drive/MyDrive/Datasets/AffectNet/train_class/\"\n",
    "subfolders = [el for el in os.listdir(base_dir)]\n",
    "classes = [cl for cl in os.listdir(x)]\n",
    "for cl in classes:\n",
    "  for folder in subfolders:\n",
    "    os.mkdir(base_dir  + folder + \"/\" + cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UuGW3iNaH26"
   },
   "outputs": [],
   "source": [
    "# dump balanced dataset to VFER from AVFER samples.\n",
    "source_folder = r\"/content/drive/MyDrive/Datasets/AVFER/\"\n",
    "destination_folder = r\"/content/drive/MyDrive/Datasets/VFER/\"\n",
    "cap_class = 20000\n",
    "print('Start dataset copying...')\n",
    "print('-'*40)\n",
    "# fetch all files.\n",
    "for subfolder in os.listdir(source_folder):\n",
    "  if subfolder == \".ipynb_checkpoints\":\n",
    "      print('Passing checkpoint files')\n",
    "      continue\n",
    "  print(\"Going in the subfolder {}\".format(subfolder))\n",
    "  for cl in os.listdir(source_folder + subfolder + \"/\"):\n",
    "    print(\"Going in the class {}/{}\".format(subfolder, cl))\n",
    "    if cl == \".ipynb_checkpoints\":\n",
    "      print('Passing checkpoint files')\n",
    "      continue\n",
    "    else:\n",
    "      counter_file = 0\n",
    "      for file_name in glob.glob(source_folder  + subfolder + \"/\" + cl + \"/*.jpg\"):\n",
    "        # construct full file path.\n",
    "        source = source_folder + subfolder + \"/\" + cl + \"/\" + os.path.basename(file_name)\n",
    "        destination = destination_folder + subfolder + \"/\" + cl + \"/\" + os.path.basename(file_name)\n",
    "        # copy only files.\n",
    "        if os.path.isfile(source) and counter_file < cap_class:\n",
    "          counter_file += 1\n",
    "          shutil.copy(source, destination)\n",
    "        if counter_file > cap_class:\n",
    "          break\n",
    "  \n",
    "      print('Copied on {}'.format(cl))\n",
    "print('-'*40)\n",
    "print('Dump done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okzHAa1UaUhl"
   },
   "outputs": [],
   "source": [
    "# plot final training set.\n",
    "plot_dataset(\"/content/drive/MyDrive/Datasets/VFER/train/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwn9vjS0aZKW"
   },
   "outputs": [],
   "source": [
    "# plot final validation set.\n",
    "plot_dataset(\"/content/drive/MyDrive/Datasets/VFER/val/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y67zGY39acHT"
   },
   "outputs": [],
   "source": [
    "# plot final testing set.\n",
    "plot_dataset(\"/content/drive/MyDrive/Datasets/VFER/test/*\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMiUKov7Ura+R2clwLyxpkI",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
